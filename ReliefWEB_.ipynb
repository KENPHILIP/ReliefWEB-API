{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca443a0e",
      "metadata": {
        "id": "ca443a0e"
      },
      "outputs": [],
      "source": [
        "!pip install requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fpdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE16UC952yPO",
        "outputId": "ab872383-61ca-4493-dd58-d398f1dc7268"
      },
      "id": "wE16UC952yPO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40702 sha256=ffe9dbd4bfa3c6014b02f2243bcf8c8bd49ac1901222f1c68a77fb339a3efdc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ReliefWeb API endpoint\n",
        "url = \"https://api.reliefweb.int/v1/disasters\"\n",
        "\n",
        "# ICPAC countries\n",
        "icpac_countries = [\"Burundi\", \"Djibouti\", \"Eritrea\", \"Ethiopia\", \"Kenya\", \"Rwanda\", \"Somalia\", \"South Sudan\", \"Sudan\", \"Uganda\", \"Tanzania\"]\n",
        "\n",
        "# Prepare the payload for the POST request\n",
        "payload = {\n",
        "    \"filter\": {\n",
        "        \"operator\": \"OR\",\n",
        "        \"conditions\": []\n",
        "    },\n",
        "    \"fields\": {\n",
        "        \"include\": [\n",
        "            \"id\", \"name\", \"country\", \"date\", \"description\"\n",
        "        ]\n",
        "    },\n",
        "    \"limit\": 100,\n",
        "    \"sort\": [\"date.created:desc\"]\n",
        "}\n",
        "\n",
        "# Add conditions for each country\n",
        "for country in icpac_countries:\n",
        "    payload[\"filter\"][\"conditions\"].append({\n",
        "        \"field\": \"country.name\",\n",
        "        \"value\": country\n",
        "    })\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Accept\": \"application/json\"\n",
        "}\n",
        "\n",
        "# POST request to fetch the data\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "# Print response content\n",
        "print(\"Response Status Code:\", response.status_code)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    disaster_data = response.json()\n",
        "    disasters = disaster_data.get('data', [])\n",
        "\n",
        "    # Function to extract data from description\n",
        "    def extract_data_from_description(description):\n",
        "        data = {\n",
        "            \"total_affected\": \"Data not available\",\n",
        "            \"total_injured\": \"Data not available\",\n",
        "            \"total_missing\": \"Data not available\",\n",
        "            \"total_displaced\": \"Data not available\",\n",
        "            \"economic_impact\": \"Data not available\",\n",
        "            \"total_damaged_houses\": \"Data not available\",\n",
        "            \"total_damaged_schools\": \"Data not available\",\n",
        "            \"total_damaged_health_facilities\": \"Data not available\"\n",
        "        }\n",
        "\n",
        "        # Use regular expressions to find the relevant numbers\n",
        "        total_affected = re.search(r\"(\\d+,\\d+|\\d+)\\s*people have been affected\", description)\n",
        "        total_injured = re.search(r\"(\\d+,\\d+|\\d+)\\s*injured\", description)\n",
        "        total_missing = re.search(r\"(\\d+,\\d+|\\d+)\\s*missing\", description)\n",
        "        total_displaced = re.search(r\"(\\d+,\\d+|\\d+)\\s*displaced\", description)\n",
        "\n",
        "        if total_affected:\n",
        "            data[\"total_affected\"] = total_affected.group(1)\n",
        "        if total_injured:\n",
        "            data[\"total_injured\"] = total_injured.group(1)\n",
        "        if total_missing:\n",
        "            data[\"total_missing\"] = total_missing.group(1)\n",
        "        if total_displaced:\n",
        "            data[\"total_displaced\"] = total_displaced.group(1)\n",
        "\n",
        "        return data\n",
        "\n",
        "    # Function to scrape additional data from URLs\n",
        "    def scrape_additional_data(url):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                text = soup.get_text()\n",
        "\n",
        "                # Similar extraction logic from the scraped text\n",
        "                data = {\n",
        "                    \"total_affected\": \"Data not available\",\n",
        "                    \"total_injured\": \"Data not available\",\n",
        "                    \"total_missing\": \"Data not available\",\n",
        "                    \"total_displaced\": \"Data not available\",\n",
        "                    \"economic_impact\": \"Data not available\",\n",
        "                    \"total_damaged_houses\": \"Data not available\",\n",
        "                    \"total_damaged_schools\": \"Data not available\",\n",
        "                    \"total_damaged_health_facilities\": \"Data not available\"\n",
        "                }\n",
        "\n",
        "                total_affected = re.search(r\"(\\d+,\\d+|\\d+)\\s*people have been affected\", text)\n",
        "                total_injured = re.search(r\"(\\d+,\\d+|\\d+)\\s*injured\", text)\n",
        "                total_missing = re.search(r\"(\\d+,\\d+|\\d+)\\s*missing\", text)\n",
        "                total_displaced = re.search(r\"(\\d+,\\d+|\\d+)\\s*displaced\", text)\n",
        "\n",
        "                if total_affected:\n",
        "                    data[\"total_affected\"] = total_affected.group(1)\n",
        "                if total_injured:\n",
        "                    data[\"total_injured\"] = total_injured.group(1)\n",
        "                if total_missing:\n",
        "                    data[\"total_missing\"] = total_missing.group(1)\n",
        "                if total_displaced:\n",
        "                    data[\"total_displaced\"] = total_displaced.group(1)\n",
        "\n",
        "                return data\n",
        "            else:\n",
        "                return {}\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {url}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    # Extract the required data\n",
        "    extracted_data = []\n",
        "    for disaster in disasters:\n",
        "        fields = disaster['fields']\n",
        "        description = fields.get(\"description\", \"\")\n",
        "        data_from_description = extract_data_from_description(description)\n",
        "\n",
        "        # Check for URLs in the description to scrape additional data\n",
        "        urls = re.findall(r'(https?://[^\\s]+)', description)\n",
        "        additional_data = {}\n",
        "        for url in urls:\n",
        "            additional_data = scrape_additional_data(url)\n",
        "            if additional_data:\n",
        "                break  # Use the first successful data extraction\n",
        "\n",
        "        # Merge data from description and additional data\n",
        "        final_data = {**data_from_description, **additional_data}\n",
        "\n",
        "        record = {\n",
        "            \"id\": disaster[\"id\"],\n",
        "            \"name\": fields.get(\"name\"),\n",
        "            \"country\": fields.get(\"country\")[0]['name'] if fields.get(\"country\") else \"Unknown\",\n",
        "            \"date\": fields.get(\"date\").get(\"event\"),\n",
        "            \"description\": description,\n",
        "            \"total_affected\": final_data[\"total_affected\"],\n",
        "            \"total_injured\": final_data[\"total_injured\"],\n",
        "            \"total_missing\": final_data[\"total_missing\"],\n",
        "            \"total_displaced\": final_data[\"total_displaced\"],\n",
        "            \"economic_impact\": final_data[\"economic_impact\"],\n",
        "            \"total_damaged_houses\": final_data[\"total_damaged_houses\"],\n",
        "            \"total_damaged_schools\": final_data[\"total_damaged_schools\"],\n",
        "            \"total_damaged_health_facilities\": final_data[\"total_damaged_health_facilities\"]\n",
        "        }\n",
        "        extracted_data.append(record)\n",
        "\n",
        "    # Convert the extracted data to a DataFrame\n",
        "    new_df = pd.DataFrame(extracted_data)\n",
        "    print(new_df.head())\n",
        "\n",
        "else:\n",
        "    print(\"Failed to retrieve data. Status code:\", response.status_code)\n",
        "    print(\"Response Content:\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SaTn_1xSBOD",
        "outputId": "c5b0494b-600a-4795-b432-770e23a33ed9"
      },
      "id": "0SaTn_1xSBOD",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Status Code: 200\n",
            "Failed to scrape http://www.rsmcnewdelhi.imd.gov.in/index.php?lang=en#): HTTPSConnectionPool(host='rsmcnewdelhi.imd.gov.inindex.php', port=443): Max retries exceeded with url: /?lang=en (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d97b3578820>: Failed to resolve 'rsmcnewdelhi.imd.gov.inindex.php' ([Errno -2] Name or service not known)\"))\n",
            "Failed to scrape http://www.rsmcnewdelhi.imd.gov.in/index.php?lang=en): HTTPSConnectionPool(host='rsmcnewdelhi.imd.gov.inindex.php', port=443): Max retries exceeded with url: /?lang=en) (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d97b3503ee0>: Failed to resolve 'rsmcnewdelhi.imd.gov.inindex.php' ([Errno -2] Name or service not known)\"))\n",
            "Failed to scrape http://www.met.gov.om/opencms/export/sites/default/dgman/en/home/): HTTPConnectionPool(host='www.met.gov.om', port=80): Max retries exceeded with url: /opencms/export/sites/default/dgman/en/home/) (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d97b3501750>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "      id                         name                      country  \\\n",
            "0  52004  Ethiopia: Floods - May 2024                     Ethiopia   \n",
            "1  52002    Uganda: Floods - May 2024                       Uganda   \n",
            "2  51991    Rwanda: Floods - Apr 2024                       Rwanda   \n",
            "3  51982  Tanzania: Floods - Apr 2024  United Republic of Tanzania   \n",
            "4  51980   Somalia: Floods - Apr 2024                      Somalia   \n",
            "\n",
            "                        date  \\\n",
            "0  2024-05-08T00:00:00+00:00   \n",
            "1  2024-05-01T00:00:00+00:00   \n",
            "2  2024-04-20T00:00:00+00:00   \n",
            "3  2024-04-07T00:00:00+00:00   \n",
            "4  2024-04-19T00:00:00+00:00   \n",
            "\n",
            "                                         description      total_affected  \\\n",
            "0  Weeks of heavy rainfall and flooding in the Ea...             637,000   \n",
            "1  Since the end of April, heavy rainfall has bee...  Data not available   \n",
            "2  In Rwanda, heavy rains and floods killed 14 pe...  Data not available   \n",
            "3  Heavy rainfall continues to affect most of Tan...  Data not available   \n",
            "4  The Gu (April to June) rains intensified with ...  Data not available   \n",
            "\n",
            "        total_injured       total_missing     total_displaced  \\\n",
            "0  Data not available  Data not available             234,000   \n",
            "1  Data not available  Data not available  Data not available   \n",
            "2  Data not available  Data not available  Data not available   \n",
            "3                 236  Data not available  Data not available   \n",
            "4  Data not available  Data not available               5,130   \n",
            "\n",
            "      economic_impact total_damaged_houses total_damaged_schools  \\\n",
            "0  Data not available   Data not available    Data not available   \n",
            "1  Data not available   Data not available    Data not available   \n",
            "2  Data not available   Data not available    Data not available   \n",
            "3  Data not available   Data not available    Data not available   \n",
            "4  Data not available   Data not available    Data not available   \n",
            "\n",
            "  total_damaged_health_facilities  \n",
            "0              Data not available  \n",
            "1              Data not available  \n",
            "2              Data not available  \n",
            "3              Data not available  \n",
            "4              Data not available  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the extracted data to a DataFrame\n",
        "new_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "# Define the file path where you want to save the CSV\n",
        "csv_file_path = \"disaster_data.csv\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "new_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(\"Data saved to:\", csv_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lVXvIbTVmEP",
        "outputId": "b77475f2-227d-4d22-bbd6-d4ecb498f333"
      },
      "id": "5lVXvIbTVmEP",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to: disaster_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4e521004",
      "metadata": {
        "id": "4e521004"
      },
      "outputs": [],
      "source": [
        "!pip install solara altair vega_datasets requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8iRoCTntNWC",
        "outputId": "413b6a8d-0800-4559-c47a-13dbe9a9865b"
      },
      "id": "T8iRoCTntNWC",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.35.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.1)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ab9fc189",
      "metadata": {
        "id": "ab9fc189"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "from vega_datasets import data\n",
        "import streamlit as st\n",
        "\n",
        "# Create a dataframe from the fetched disaster data\n",
        "def create_dataframe(disasters):\n",
        "    records = []\n",
        "    for disaster in disasters:\n",
        "        fields = disaster.get('fields', {})\n",
        "\n",
        "        record = {\n",
        "            'id': disaster.get('id', 'N/A'),\n",
        "            'name': disaster.get('name', 'N/A'),\n",
        "            'country': fields.get('country', [{}])[0].get('name', 'Unknown'),\n",
        "            'date': fields.get('date', {}).get('created', 'N/A'),\n",
        "            'description': fields.get('description', 'N/A'),\n",
        "            'deaths': fields.get('deaths', 0),\n",
        "            'affected': fields.get('affected', 0),\n",
        "            'injured': fields.get('injured', 0),\n",
        "            'missing': fields.get('missing', 0),\n",
        "            'displaced': fields.get('displaced', 0),\n",
        "            'economic_cost': fields.get('economic_cost', 0),\n",
        "            'damaged_houses': fields.get('damaged_houses', 0),\n",
        "            'damaged_schools': fields.get('damaged_schools', 0),\n",
        "            'damaged_health_facilities': fields.get('damaged_health_facilities', 0)\n",
        "        }\n",
        "        records.append(record)\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Create a dataframe\n",
        "df = create_dataframe(disasters)\n",
        "\n",
        "# Define a function to create an Altair chart\n",
        "def create_chart(df):\n",
        "    world_map = alt.topo_feature(data.world_110m.url, 'countries')\n",
        "\n",
        "    chart = alt.Chart(world_map).mark_geoshape().encode(\n",
        "        color=alt.Color('deaths:Q', title='Deaths')\n",
        "    ).transform_lookup(\n",
        "        lookup='properties.name',\n",
        "        from_=alt.LookupData(df, 'country', ['deaths'])\n",
        "    ).project(\n",
        "        type='mercator'\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=400\n",
        "    )\n",
        "\n",
        "    return chart\n",
        "\n",
        "# Create a Streamlit app\n",
        "def main():\n",
        "    st.title(\"Disaster Dashboard\")\n",
        "    st.header(\"Map of Disasters\")\n",
        "    chart = create_chart(df)\n",
        "    st.altair_chart(chart)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd path/to/your/directory\n",
        "streamlit run disaster_dashboard.py\n"
      ],
      "metadata": {
        "id": "ll5ordqqt1Kz"
      },
      "id": "ll5ordqqt1Kz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVLcVE0p-NuU",
        "outputId": "ecad1635-54f1-4e56-ca64-9e1a08109b93"
      },
      "id": "rVLcVE0p-NuU",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: httpcore, httpx, openai\n",
            "Successfully installed httpcore-1.0.5 httpx-0.27.0 openai-1.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2114b913",
      "metadata": {
        "id": "2114b913"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}