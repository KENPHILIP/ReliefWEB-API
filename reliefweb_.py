# -*- coding: utf-8 -*-
"""ReliefWEB .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQHwPjwpcpH85QrMAmEmaP2--FGrO0Fj

This code snippet handles various tasks related to disaster and flood data across East and Horn of Africa countries, with a focus on ICPAC (Intergovernmental Authority on Development Climate Prediction and Applications Centre) member states. It begins by retrieving flood data from an Excel file specified by `excel_url`, filters this data to include only the countries listed in `icpac_countries`, and saves the filtered dataset as `filtered_icpac_disaster_data.csv`.

Utilizes the ReliefWeb API to fetch detailed disaster information for each ICPAC country. It constructs POST requests with specific parameters to retrieve fields such as disaster ID, name, country, date, description, and other relevant attributes. The retrieved data is aggregated into a list (`all_data_list`) and converted into a pandas DataFrame (`df_reliefweb`), which is subsequently saved as `disasters_data_icpac.csv`.
"""

#!pip install requests --quiet

"""Data retrieval and procesing"""

import re
import json
import requests
import pandas as pd
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from datetime import datetime

#pip install openpyxl

# give a code to clear the folder
#!rm -rf /content/DTM_IOM_datasets

#from google.colab import drive
#drive.mount('/content/drive')

# URL for the Excel file
excel_url = "https://floodobservatory.colorado.edu/temp/FloodArchive.xlsx"

# Read the Excel file
df_excel = pd.read_excel(excel_url)

# List of 11 ICPAC countries
icpac_countries = [
    'Burundi', 'Djibouti', 'Eritrea', 'Ethiopia', 'Kenya',
    'Rwanda', 'Somalia', 'South Sudan', 'Sudan', 'Tanzania', 'Uganda'
]

# Filter data to include only the ICPAC countries
df_excel_icpac = df_excel[df_excel['Country'].isin(icpac_countries)]

# Save the filtered dataframe to a new CSV file
df_excel_icpac.to_csv('filtered_icpac_disaster_data.csv', index=False)

# Display the last few rows of the filtered dataframe as a confirmation
print(df_excel_icpac.tail())



import requests
import pandas as pd
from pandas import json_normalize

# List of ICPAC countries
icpac_countries = [
    "Djibouti", "Eritrea", "Ethiopia", "Kenya", "Somalia", "South Sudan",
    "Sudan", "Uganda", "Burundi", "Rwanda", "Tanzania"
]

# Initialize an empty list to store data
all_data_list = []

# Loop through each country and retrieve data
for country in icpac_countries:
    # Prepare the payload for the POST request
    payload = {
        "filter": {
            "operator": "AND",
            "conditions": [
                {"field": "country.name", "value": country}
            ]
        },
        "fields": {
            "include": ["id", "name", "country", "date", "description", "type", "status", "glide", "url"]
        },
        "limit": 100,
        "sort": ["date:desc"]
    }

    headers = {"Content-Type": "application/json", "Accept": "application/json"}

    # POST request
    response = requests.post("https://api.reliefweb.int/v1/disasters", headers=headers, json=payload, timeout=60)

    # Print response content for debugging
    if response.status_code != 200:
        print(f"Failed to retrieve data for {country}: {response.status_code}")
        print("Response Content:", response.text)
        continue

    disaster_data = response.json()
    disasters = disaster_data.get("data", [])

    # Extract and process the data
    for disaster in disasters:
        fields = disaster.get("fields", {})
        description = fields.get("description", "Data not available")
        country_info = fields.get("country", [{"name": "N/A"}])[0]
        country_name = country_info.get("name", "N/A")

        # Prepare data for each disaster
        data = {
            "id": fields.get("id", "N/A"),
            "name": fields.get("name", "N/A"),
            "country": country_name,
            "date": fields.get("date", "N/A"),
            "description": description,
            "type": fields.get("type", "N/A"),
            "status": fields.get("status", "N/A"),
            "glide": fields.get("glide", "N/A"),
            "url": fields.get("url", "N/A")
        }
        all_data_list.append(data)

# Create dataframe from the combined data list
df_reliefweb = pd.DataFrame(all_data_list)

# Ensure 'date' column is in string format
df_reliefweb['date'] = df_reliefweb['date'].astype(str)

# Extract the date part and split into year, month, and day
if 'date' in df_reliefweb.columns:
    # Extract date part (YYYY-MM-DD) from ISO 8601 timestamp
    df_reliefweb['date'] = df_reliefweb['date'].str.split('T', expand=True)[0]

    # Split the date part into separate columns for year, month, and day
    df_reliefweb[['year', 'month', 'day']] = df_reliefweb['date'].str.split('-', expand=True)

# Normalize and explode 'type' column
if 'type' in df_reliefweb.columns:
    # Normalize 'type' column (convert list of dicts to separate rows)
    type_normalized = df_reliefweb['type'].apply(pd.json_normalize)

    # Combine normalized 'type' data into a single DataFrame
    type_expanded = pd.concat(type_normalized.tolist(), ignore_index=True)

    # Rename columns in type_expanded to avoid overlap
    type_expanded.columns = [f"type_{col}" if col in df_reliefweb.columns else col for col in type_expanded.columns]

    # Concatenate the original dataframe with the expanded 'type' DataFrame
    df_reliefweb = df_reliefweb.drop(columns=['type']).join(type_expanded)
else:
    print("Type column is missing.")

# Save to disasters_data_icpac.csv
df_reliefweb.to_csv('disasters_data_icpac.csv', index=False)

print("Data has been successfully processed and saved.")

# URL for the HTML page
html_url = "https://floodobservatory.colorado.edu/Version3/MasterListrev.htm"

# Make the request and parse HTML
response_html = requests.get(html_url)
soup = BeautifulSoup(response_html.content, 'html.parser')

# Example: Extracting all links from the HTML
links = soup.find_all('a')
for link in links:
    print(link.get('href'))

df_reliefweb.columns

# Read the CSV file into a DataFrame
#df_emdat = pd.read_csv("/content/public_emdat_custom_request_2024-07-11_aa4cbcb5-2596-4ad8-84fc-bd502aa8b051.csv")

# Print the first few rows of the DataFrame
#print(df_emdat.head())