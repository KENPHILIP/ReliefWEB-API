# -*- coding: utf-8 -*-
"""ReliefWEB .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQHwPjwpcpH85QrMAmEmaP2--FGrO0Fj
"""

!pip install requests --quiet

"""Data retrieval and procesing"""

import json
import requests
import pandas as pd

# ReliefWeb API endpoint
url = "https://api.reliefweb.int/v1/disasters"

# ICPAC countries
icpac_countries = [
    "Burundi", "Djibouti", "Eritrea", "Ethiopia", "Kenya", "Rwanda",
    "Somalia", "South Sudan", "Sudan", "Uganda", "Tanzania"
]

# Prepare the payload for the POST request
payload = {
    "filter": {"operator": "OR", "conditions": []},
    "fields": {"include": ["id", "name", "country", "date", "description", "type"]},
    "limit": 100,
    "sort": ["date:desc"]
}

# Add conditions for each country
for country in icpac_countries:
    payload["filter"]["conditions"].append({"field": "country.name", "value": country})

headers = {"Content-Type": "application/json", "Accept": "application/json"}

# POST request
response = requests.post(url, headers=headers, json=payload, timeout=60)

# Check response status
if response.status_code == 200:
    disaster_data = response.json()
    disasters = disaster_data.get("data", [])

    # Extract and process the data
    data_list = []
    for disaster in disasters:
        fields = disaster["fields"]
        description = fields.get("description", "Data not available")
        country_info = fields.get("country", [{"name": "N/A"}])[0]
        country_name = country_info.get("name", "N/A")
        # Extract coordinates from the country field
        coordinates = country_info.get("location", {}).get("geometry", {}).get("coordinates", [])
        latitude = coordinates[1] if len(coordinates) > 1 else "N/A"
        longitude = coordinates[0] if coordinates else "N/A"
        data = {
            "id": fields.get("id", "N/A"),
            "name": fields.get("name", "N/A"),
            "country": country_name,
            "date": fields.get("date", "N/A"),
            "description": description,
            "disaster_type": fields.get("type", "N/A"),
            "latitude": latitude,
            "longitude": longitude
        }
        data_list.append(data)

    # Convert to DataFrame
    df = pd.DataFrame(data_list)
    df.to_csv('disasters_data.csv', index=False)
else:
    print("Failed to retrieve data. Status code:", response.status_code)
    print("Response Content:", response.content)

import pandas as pd

# Load the data
df = pd.read_csv('disasters_data.csv')

# Function to extract event date from the nested dictionary
def extract_event_date(date_dict_str):
    try:
        date_dict = eval(date_dict_str)
        return date_dict.get('event', 'N/A')
    except (SyntaxError, ValueError):
        return 'N/A'

# Apply the function to extract event dates
df['event_date'] = df['date'].apply(extract_event_date)

# Convert event_date to datetime
df['event_date'] = pd.to_datetime(df['event_date'], errors='coerce')

# Drop rows with invalid dates
df = df.dropna(subset=['event_date'])

# Extract year, month, and day as features
df['year'] = df['event_date'].dt.year
df['month'] = df['event_date'].dt.month
df['day'] = df['event_date'].dt.day

# Drop the original date and description columns
df = df.drop(columns=['date', 'description', 'event_date'])

# Drop the country columns
country_columns = [col for col in df.columns if col.startswith('country_')]
df = df.drop(columns=country_columns)

# Drop the longitude and latitude columns
df = df.drop(columns=['longitude', 'latitude'])

# Save the processed data
df.to_csv('processed_disasters_data.csv', index=False)

print(df.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style
sns.set_style("whitegrid")

# Read the CSV file into a DataFrame
disasters_data = pd.read_csv('processed_disasters_data.csv')

# List of 11 countries in ICPAC
icpac_countries = ['Burundi', 'Djibouti', 'Eritrea', 'Ethiopia', 'Kenya',
                   'Rwanda', 'Somalia', 'South Sudan', 'Sudan', 'Tanzania', 'Uganda']

# Filter data for only the 11 ICPAC countries
icpac_data = disasters_data[disasters_data['country'].isin(icpac_countries)]

# Grouping data by both country and disaster type
grouped_data = icpac_data.groupby(['country', 'disaster_type']).size().unstack(fill_value=0)

# Plotting disasters for each country in ICPAC by disaster type
plt.figure(figsize=(12, 8))
palette = sns.color_palette("husl", len(grouped_data.columns))
grouped_data.plot(kind='barh', stacked=True, color=palette)
plt.xlabel('Number of Disasters', fontsize=14)
plt.ylabel('Country', fontsize=14)
plt.title('Disasters in ICPAC Countries by Type', fontsize=16)
plt.legend(title='Disaster Type', fontsize=12, bbox_to_anchor=(1.05, 1))
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

"""feature engineering

##MODEL TRAINING
"""

# Print a sample of the "disaster_type" column
print(df['disaster_type'].head())

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best parameters found by GridSearchCV
print("Best Parameters:", grid_search.best_params_)

# Evaluate the best model on the test set
best_rf_model = grid_search.best_estimator_
y_pred = best_rf_model.predict(X_test)
print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""Model Testing and Evaluation"""

# Select only numeric columns
numeric_cols = icpac_data.select_dtypes(include=['int64', 'float64'])

# Calculate the correlation matrix
corr_matrix = numeric_cols.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

"""Save the Model for Deployment"""

from joblib import dump

# Save the trained model
model_filename = 'random_forest_model.joblib'
dump(rf_classifier, model_filename)

print("Model saved successfully as", model_filename)

from joblib import load

# Load the saved model
loaded_model = load(model_filename)

pip install fastapi

pip install openai