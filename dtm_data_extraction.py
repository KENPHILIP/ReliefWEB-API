# -*- coding: utf-8 -*-
"""DTM_DATA_EXTRACTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WCR4kbBItDF2XEegWMdUtlM2pfYOb1Te

The script automates the retrieval and organization of DTM (Displacement Tracking Matrix) datasets from the IOM (International Organization for Migration) API for countries in East and Horn of Africa associated specifically for ICPAC regions (Intergovernmental Authority on Development Climate Prediction and Applications Centre). It begins by defining necessary imports and setting up endpoint URLs, country lists, and dataset components. The main functionality includes two primary functions: `download_country_datasets` and `display_file_titles`.

- `download_country_datasets` downloads datasets for specified countries and year ranges, constructing URLs tailored to each country and including specified dataset components. It makes GET requests to fetch the data and saves each dataset as an Excel file in a structured directory (`./DTM_IOM_datasets/{country}`). Error handling ensures reliability during data retrieval.

- `display_file_titles` reads and displays the sheet titles of downloaded Excel files for a given country, utilizing `openpyxl` for file manipulation. It handles exceptions gracefully in case of reading errors.

The `main` function coordinates the entire process by creating a main folder (`./DTM_IOM_datasets`) to store all downloaded data. It iterates through each country in the defined list (`east_horn_africa_countries`), calling `download_country_datasets` to fetch datasets and `display_file_titles` to show their details.
"""

#!rm -rf /content/DTM_IOM_datasets



import os
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import openpyxl

# Define the base URL and the URL to start scraping from
base_url = "https://dtm.iom.int"
start_url = "https://dtm.iom.int/datasets"

# List of East and Horn of Africa countries in ICPAC
east_horn_africa_countries = [
    "Burundi", "Djibouti", "Eritrea", "Ethiopia", "Kenya",
    "Rwanda", "Somalia", "South Sudan", "Sudan", "Tanzania", "Uganda"
]

# Create a main folder for all datasets
output_folder = "./DTM_IOM_datasets"
os.makedirs(output_folder, exist_ok=True)

# Function to create subfolders for each country
def create_country_folders():
    for country in east_horn_africa_countries:
        country_folder = os.path.join(output_folder, country)
        os.makedirs(country_folder, exist_ok=True)
    print("Created folders for all countries")

# Function to fetch and download datasets from the webpage
def fetch_datasets():
    print(f"Fetching datasets from: {start_url}")
    try:
        response = requests.get(start_url)
        response.raise_for_status()  # Raise an exception for HTTP errors
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract links to dataset pages for each country
        links = soup.find_all('a', href=True)
        print(f"Found {len(links)} links on the main page.")
        for link in links:
            href = link['href']
            text = link.text.strip()
            for country in east_horn_africa_countries:
                if country.lower() in text.lower():
                    filtered_url = urljoin(base_url, href)
                    print(f"Found link for {country}: {filtered_url}")
                    fetch_country_datasets(filtered_url, country)

    except requests.exceptions.RequestException as e:
        print(f"Error fetching datasets from {start_url}: {e}")

def fetch_country_datasets(url, country):
    print(f"Fetching datasets for {country} from: {url}")
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract dataset links
        links = soup.find_all('a', href=True)
        print(f"Found {len(links)} links on the page for {country}.")
        for link in links:
            href = link['href']
            text = link.text.strip()
            # Only download .xlsx files
            if href.lower().endswith('.xlsx') or "infosheet" in text.lower():
                full_url = urljoin(base_url, href)
                filename = os.path.basename(href)
                file_base_name = os.path.splitext(filename)[0]
                country_folder = os.path.join(output_folder, country)

                # Create a subfolder for files with the same base name
                file_folder = os.path.join(country_folder, file_base_name)
                os.makedirs(file_folder, exist_ok=True)

                filepath = os.path.join(file_folder, filename)
                try:
                    file_response = requests.get(full_url)
                    file_response.raise_for_status()
                    with open(filepath, 'wb') as f:
                        f.write(file_response.content)
                    print(f"Downloaded {filename} for {country}")
                except requests.exceptions.RequestException as e:
                    print(f"Error downloading {filename} for {country}: {e}")
                    continue

    except requests.exceptions.RequestException as e:
        print(f"Error fetching datasets for {country} from {url}: {e}")

# Function to read and verify the content of downloaded Excel files
def read_excel_sheets(filepath, country):
    try:
        wb = openpyxl.load_workbook(filepath, read_only=True)
        for sheet_name in wb.sheetnames:
            sheet = wb[sheet_name]
            first_cell_value = sheet.cell(row=1, column=1).value
            print(f"Read file: {filepath} - Sheet: {sheet_name} - First cell value: {first_cell_value} - Country: {country}")

    except Exception as e:
        print(f"Error reading {filepath}: {e}")

# Main function to initiate the dataset fetching process
def main():
    create_country_folders()
    fetch_datasets()

if __name__ == "__main__":
    main()

# downloading the folder

!zip -r DTM_IOM_datasets.zip /content/DTM_IOM_datasets
from google.colab import files
files.download('DTM_IOM_datasets.zip')

