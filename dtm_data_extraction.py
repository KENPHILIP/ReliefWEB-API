# -*- coding: utf-8 -*-
"""DTM_DATA_EXTRACTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WCR4kbBItDF2XEegWMdUtlM2pfYOb1Te

The script automates the retrieval and organization of DTM (Displacement Tracking Matrix) datasets from the IOM (International Organization for Migration) API for countries in East and Horn of Africa associated specifically for ICPAC regions (Intergovernmental Authority on Development Climate Prediction and Applications Centre). It begins by defining necessary imports and setting up endpoint URLs, country lists, and dataset components. The main functionality includes two primary functions: `download_country_datasets` and `display_file_titles`.

- `download_country_datasets` downloads datasets for specified countries and year ranges, constructing URLs tailored to each country and including specified dataset components. It makes GET requests to fetch the data and saves each dataset as an Excel file in a structured directory (`./DTM_IOM_datasets/{country}`). Error handling ensures reliability during data retrieval.

- `display_file_titles` reads and displays the sheet titles of downloaded Excel files for a given country, utilizing `openpyxl` for file manipulation. It handles exceptions gracefully in case of reading errors.

The `main` function coordinates the entire process by creating a main folder (`./DTM_IOM_datasets`) to store all downloaded data. It iterates through each country in the defined list (`east_horn_africa_countries`), calling `download_country_datasets` to fetch datasets and `display_file_titles` to show their details.
"""

import os
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import openpyxl

# Define the base URL and the URL to start scraping from
base_url = "https://dtm.iom.int"
start_url = "https://dtm.iom.int/datasets"

# List of East and Horn of Africa countries in ICPAC
east_horn_africa_countries = [
    "Burundi", "Djibouti", "Eritrea", "Ethiopia", "Kenya",
    "Rwanda", "Somalia", "South Sudan", "Sudan", "Tanzania", "Uganda"
]

# Create a main folder for all datasets
output_folder = "./DTM_IOM_datasets"
os.makedirs(output_folder, exist_ok=True)

# Function to create subfolders for each country
def create_country_folders():
    for country in east_horn_africa_countries:
        country_folder = os.path.join(output_folder, country)
        os.makedirs(country_folder, exist_ok=True)

# Function to fetch and download datasets from the webpage
def fetch_datasets():
    try:
        response = requests.get(start_url)
        response.raise_for_status()  # Raise an exception for HTTP errors
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract dataset links
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            if href.lower().endswith('.xlsx'):  # Only download .xlsx files
                full_url = urljoin(base_url, href)
                filename = os.path.basename(href)
                for country in east_horn_africa_countries:
                    country_folder = os.path.join(output_folder, country)
                    filepath = os.path.join(country_folder, filename)
                    try:
                        file_response = requests.get(full_url)
                        file_response.raise_for_status()
                        with open(filepath, 'wb') as f:
                            f.write(file_response.content)
                        print(f"Downloaded {filename} for {country}")
                        # Read all sheets from the downloaded Excel file
                        if filename.lower().endswith('.xlsx'):
                            read_excel_sheets(filepath, country)

                    except requests.exceptions.RequestException as e:
                        print(f"Error downloading {filename} for {country}: {e}")
                        continue

    except requests.exceptions.RequestException as e:
        print(f"Error fetching datasets from {start_url}: {e}")

# Function to read and verify the content of downloaded Excel files
def read_excel_sheets(filepath, country):
    try:
        wb = openpyxl.load_workbook(filepath, read_only=True)
        for sheet_name in wb.sheetnames:
            sheet = wb[sheet_name]
            first_cell_value = sheet.cell(row=1, column=1).value
            print(f"Read file: {filepath} - Sheet: {sheet_name} - First cell value: {first_cell_value} - Country: {country}")

            # Check if the sheet is an info sheet or matches any dataset types
            if any(dataset_type in sheet_name.lower() for dataset_type in dataset_types) or "info" in sheet_name.lower():
                print(f"Found dataset type info or relevant dataset: {sheet_name}")

    except Exception as e:
        print(f"Error reading {filepath}: {e}")

# Main function to initiate the dataset fetching process
def main():
    create_country_folders()
    fetch_datasets()

if __name__ == "__main__":
    main()

